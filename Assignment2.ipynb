{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e53bf5c7",
      "metadata": {
        "id": "e53bf5c7"
      },
      "source": [
        "# Assignment 2: Implementing Decoding Strategies for Summarization (40 points)\n",
        "\n",
        "In this assignment, you will **implement decoding algorithms** used for text summarization using a pretrained Transformer model.\n",
        "\n",
        "---\n",
        "\n",
        "### Your Task\n",
        "1. **Load** the `sshleifer/distilbart-cnn-12-6` summarization model.\n",
        "2. **Implement the following decoding strategies from scratch** (no `.generate()` allowed!). You need to provide your own explanation on your implementation for each function:\n",
        "   - Greedy decoding (3 points)\n",
        "   - Top-k sampling (3 points)\n",
        "   - Top-p (nucleus) sampling (3 points)\n",
        "   - Beam search (3 points)\n",
        "   - Beam search with n-gram blocking (3 points)\n",
        "3. Use your decoder to summarize 200 articles from the CNN/DailyMail dataset.\n",
        "4. Implement the following ROUGE metrics and evaluate your summaries using your own ROUGE metric implementation.\n",
        "  - Implementation\n",
        "    - ROUGE-n (2 points): e.g., ROUGE-1 & ROUGE-2\n",
        "    - ROUGE-L (4 points)\n",
        "  - Explanation on your implementation (2 points)\n",
        "  - Discuss how to improve these metrics to perform a better evaluation? (2 points)\n",
        "5. Discussion (15 points)\n",
        "\n",
        "---\n",
        "\n",
        "**Note:**\n",
        "  - Regarding the decoding strategies, you are expected to work directly with model logits and sampling logic. Do not use `model.generate()` or any pre-built function. **Hint**: use \"outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\"\n",
        "  - For each function, you need to write clear and concise comments about your implementation. This may not be line-by-line, but rather meaningful chunk of codes.\n",
        "  - For each question, justify your answer with explanations.\n",
        "\n",
        "### **We highly recommend using 'cpu' as the default for development, and switching to 'gpu' only for evaluating summarization performance. This is due to the limited GPU availability in the Colab environment.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b788c8",
      "metadata": {
        "id": "f3b788c8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2b0b6c5",
      "metadata": {
        "id": "f2b0b6c5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import json\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71208984",
      "metadata": {},
      "source": [
        "In this assignment, we use the CNN/DailyMail summarization dataset, a widely-used benchmark for training and evaluating text summarization models.\n",
        "\n",
        "Each data sample consists of:\n",
        "\n",
        "- article: A news story, usually between 300 and 800 words.\n",
        "\n",
        "- highlights: A bullet-style abstractive summary of the article, written by human editors.\n",
        "\n",
        "This dataset is ideal for testing decoding strategies because:\n",
        "\n",
        "- The summaries are relatively short and factual\n",
        "\n",
        "- The dataset is large enough to support diverse decoding behavior\n",
        "\n",
        "- It has been used in many summarization papers, making ROUGE scores easy to compare\n",
        "\n",
        "In this assignment, we will use a small subset of 200 samples from the validation set for quick experimentation and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d2d74e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1d2d74e",
        "outputId": "597c62b4-ac8e-4b93-9373-46713d97de4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 200 samples.\n",
            "\n",
            "Example article:\n",
            "(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court's decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.\n",
            "\n",
            "Reference summary:\n",
            "Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\n",
            "Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n"
          ]
        }
      ],
      "source": [
        "# Upload cnn_dm_200.json file\n",
        "\n",
        "# Load 200-sample dataset\n",
        "with open(\"cnn_dm_200.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(dataset)} samples.\")\n",
        "print(\"\\nExample article:\")\n",
        "print(dataset[0][\"article\"])\n",
        "print(\"\\nReference summary:\")\n",
        "print(dataset[0][\"highlights\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "491ce644",
      "metadata": {},
      "source": [
        "## Decoding in Language Models\n",
        "Pretrained language models like BART or GPT generate text by predicting the most likely next token one at a time.\n",
        "However, simply choosing the most probable token at each step often leads to deterministic, repetitive, or generic outputs.\n",
        "\n",
        "To overcome this, several decoding strategies have been proposed to balance:\n",
        "\n",
        "- fluency vs. diversity\n",
        "\n",
        "- coherence vs. novelty\n",
        "\n",
        "In this assignment, you will implement five widely-used decoding strategies from scratch and compare their effects."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75e0c151",
      "metadata": {
        "id": "75e0c151"
      },
      "source": [
        "### Greedy Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tKel58QFgfKa",
      "metadata": {
        "id": "tKel58QFgfKa"
      },
      "outputs": [],
      "source": [
        "def greedy_decode(input_ids, max_length=128):\n",
        "    \"\"\"\n",
        "    Perform greedy decoding from the model using logits.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tokenized input tensor of shape [1, seq_len].\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded summary text.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Greedy Decoding from scratch\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42d72411",
      "metadata": {},
      "outputs": [],
      "source": [
        "def greedy_decode(input_ids, max_length=128):\n",
        "    \"\"\"\n",
        "    Perform greedy decoding from the model using logits.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tokenized input tensor of shape [1, seq_len].\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded summary text.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Greedy Decoding from scratch\n",
        "    input_ids = input_ids.to(device)\n",
        "    decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]]).to(device) # shape 1,1 # \n",
        "    eos_token_id = model.config.eos_token_id # take the EOS token id from the vocab\n",
        "    with torch.no_grad():\n",
        "      for _ in range(max_length):\n",
        "          print(f\"decode token {_}\")\n",
        "          output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
        "          proba_output = torch.softmax(output.logits[:,-1,:],axis=-1) # shape 1,vocab_size\n",
        "          id = torch.tensor([[torch.argmax(proba_output, axis=-1)]]).to(device) # greedy approach to take the token_id with largest probability \n",
        "\n",
        "          decoder_input_ids = torch.cat([decoder_input_ids,id],axis=-1) # append the decoded token to the generated token sequence\n",
        "      \n",
        "          if id.item() == eos_token_id: # If the model generate an EOS token, the decoding process should be stop\n",
        "              break\n",
        "        \n",
        "    summary = tokenizer.decode(decoder_input_ids[0], skip_special_tokens=False) # convert the output token sequence to string \n",
        "    \n",
        "\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AW2YSwP3PGcn",
      "metadata": {
        "id": "AW2YSwP3PGcn"
      },
      "source": [
        "Explain your implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dba10a34",
      "metadata": {
        "id": "dba10a34"
      },
      "source": [
        "### Top-k Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F8hf5mzsghUf",
      "metadata": {
        "id": "F8hf5mzsghUf"
      },
      "outputs": [],
      "source": [
        "def top_k_decode(input_ids, k=50, max_length=128, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Perform Top-k sampling decoding from the model.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tokenized input tensor of shape [1, seq_len].\n",
        "        k (int): Number of top tokens to sample from.\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "        temperature (float): Softmax temperature for sampling.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded summary text.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Top-k Sampling Decoding\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22a1957b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def top_k_decode(input_ids, k=2, max_length=128, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Perform Top-k sampling decoding from the model.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tokenized input tensor of shape [1, seq_len].\n",
        "        k (int): Number of top tokens to sample from.\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "        temperature (float): Softmax temperature for sampling.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded summary text.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Top-k Sampling Decoding\n",
        "    input_ids = input_ids.to(device)\n",
        "    decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]]).to(device) # shape 1,1 # \n",
        "    eos_token_id = model.config.eos_token_id # take the EOS token id from the vocab\n",
        "    with torch.no_grad():\n",
        "      for _ in range(max_length):\n",
        "        print(f\"decode token {_}\")\n",
        "\n",
        "        output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
        "        output_proba = torch.softmax(output.logits[:,-1,:]/temperature,axis=-1) \n",
        "        # print(f\"output_proba shape: {output_proba.shape}\")\n",
        "        values, indices = torch.topk(output_proba, k=k, axis=-1)\n",
        "        # print(indices.shape,values.shape)\n",
        "\n",
        "        \n",
        "        values = values/ torch.sum(values,axis=-1,keepdim=True).item() # redistribute proba\n",
        "        \n",
        "        local_index = torch.multinomial(values[0],num_samples=1) # sampling from redistributed distribution\n",
        "        id = indices[:,local_index]\n",
        "        \n",
        "        decoder_input_ids = torch.cat([decoder_input_ids,id],axis=-1)\n",
        "        \n",
        "        # print(decoder_input_ids[0])\n",
        "        if id.item() == eos_token_id:\n",
        "          break\n",
        "\n",
        "    summary = tokenizer.decode(decoder_input_ids[0], skip_special_tokens=False)\n",
        "    return summary\n",
        "\n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oyFRQsTiPWHn",
      "metadata": {
        "id": "oyFRQsTiPWHn"
      },
      "source": [
        "- Explain your implementation.\n",
        "- What are the role of hyperparameters of this strategy? and what happen if you change the values?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b06aad60",
      "metadata": {
        "id": "b06aad60"
      },
      "source": [
        "### Top-p Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K47uilYhgn9u",
      "metadata": {
        "id": "K47uilYhgn9u"
      },
      "outputs": [],
      "source": [
        "def top_p_decode(input_ids, p=0.9, max_length=128, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Perform Top-p (nucleus) sampling decoding from the model.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tokenized input tensor of shape [1, seq_len].\n",
        "        p (float): Cumulative probability threshold for sampling.\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "        temperature (float): Softmax temperature for sampling.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded summary text.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Top-p Sampling Decoding\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe9c4772",
      "metadata": {},
      "outputs": [],
      "source": [
        "def top_p_decode(input_ids, p=0.9, max_length=128, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Perform Top-p (nucleus) sampling decoding from the model.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tokenized input tensor of shape [1, seq_len].\n",
        "        p (float): Cumulative probability threshold for sampling.\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "        temperature (float): Softmax temperature for sampling.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded summary text.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Top-p Sampling Decoding\n",
        "    \n",
        "    input_ids = input_ids.to(device)\n",
        "    decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]]).to(device) # shape 1,1 \n",
        "    eos_token_id = model.config.eos_token_id # take the EOS token id from the vocab\n",
        "    with torch.no_grad():\n",
        "      for _ in range(max_length):\n",
        "        print(f\"decode token {_}\")\n",
        "\n",
        "        output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
        "        output_proba = torch.softmax(output.logits[:,-1,:]/temperature,axis=-1) \n",
        "        # print(output_proba.shape)\n",
        "        probs = output_proba[0]\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "        # print(sorted_probs,sorted_probs.shape)\n",
        "        # print(sorted_indices)\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
        "        # print(cumulative_probs)\n",
        "        nucleus_mask = cumulative_probs <= p\n",
        "        nucleus_mask[max((cumulative_probs > p).nonzero(as_tuple=True)[0][0], 0)] = True # Ensure we include at least one token after passing threshold\n",
        "        # print(nucleus_mask)\n",
        "        nucleus_probs = sorted_probs[nucleus_mask]\n",
        "        nucleus_indices = sorted_indices[nucleus_mask]\n",
        "        # print(nucleus_indices,nucleus_probs)\n",
        "        nucleus_probs = nucleus_probs/torch.sum(nucleus_probs)\n",
        "        # print(nucleus_probs)\n",
        "        \n",
        "        local_index = torch.multinomial(nucleus_probs,num_samples=1) # sampling from redistributed distribution\n",
        "        id = torch.tensor([[nucleus_indices[local_index]]]).to(device)\n",
        "        # print(id.shape)\n",
        "        decoder_input_ids = torch.cat([decoder_input_ids,id],axis=-1)\n",
        "\n",
        "        if id.item() == eos_token_id:\n",
        "          break\n",
        "        \n",
        "        \n",
        "    summary = tokenizer.decode(decoder_input_ids[0], skip_special_tokens=False)\n",
        "    return summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ynhylQrWPXJi",
      "metadata": {
        "id": "ynhylQrWPXJi"
      },
      "source": [
        "- Explain your implementation.\n",
        "- What are the role of hyperparameters of this strategy? and what happen if you change the values?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b274b9f",
      "metadata": {
        "id": "2b274b9f"
      },
      "source": [
        "### Beam Search Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qt52c3rSgq7b",
      "metadata": {
        "id": "Qt52c3rSgq7b"
      },
      "outputs": [],
      "source": [
        "def beam_search_decode(input_ids, beam_size=4, max_length=128):\n",
        "    \"\"\"\n",
        "    Perform beam search decoding from the model.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tokenized input tensor of shape [1, seq_len].\n",
        "        beam_size (int): Number of beams to explore.\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded summary text.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Beam Search Decoding\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9fa0a79",
      "metadata": {},
      "outputs": [],
      "source": [
        "def beam_search_decode(input_ids, beam_size=4, max_length=128):\n",
        "    \"\"\"\n",
        "    Perform beam search decoding from the model.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tokenized input tensor of shape [1, seq_len].\n",
        "        beam_size (int): Number of beams to explore.\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded summary text.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Beam Search Decoding\n",
        "    input_ids = input_ids.to(device)\n",
        "    decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]]).to(device) # shape 1,1 \n",
        "    eos_token_id = model.config.eos_token_id # take the EOS token id from the vocab\n",
        "    vocab_size = model.config.vocab_size\n",
        "    \n",
        "    top_beam = []\n",
        "    with torch.no_grad():\n",
        "      output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
        "    output = output.logits[:,-1,:]\n",
        "    output_proba = torch.softmax(output,axis=-1)\n",
        "    probas, indices = torch.topk(output_proba,k=3,axis=-1)\n",
        "    for index,proba in zip(indices[0],probas[0]):\n",
        "      print(index,proba)\n",
        "      top_beam.append((torch.tensor([[decoder_input_ids,index]]),proba.item()))\n",
        "    \n",
        "    # print(top3)\n",
        "\n",
        "    # print(top_beam)\n",
        "    for _ in range(max_length):\n",
        "      beam_list = []\n",
        "      \n",
        "      for beam in top_beam:\n",
        "        # print(beam)\n",
        "        with torch.no_grad():\n",
        "          output = model(input_ids=input_ids, decoder_input_ids=beam[0])\n",
        "        output = output.logits[:,-1,:]\n",
        "        output_proba = beam[1] * torch.softmax(output,axis=-1)\n",
        "        # print(beam[1],output_proba.shape)\n",
        "        assert output_proba.shape == output.shape, f\"shape output_proba must be [1,{vocab_size}]\"\n",
        "        for idx,local_proba in enumerate(list(output_proba[0])):\n",
        "          \n",
        "          beam_list.append((torch.cat([beam[0],torch.tensor([[idx]])],axis=-1), local_proba.item()))\n",
        "\n",
        "      sorted_beam_list = sorted(beam_list, key=lambda item: item[1], reverse=True)\n",
        "      \n",
        "      top_beam = sorted_beam_list[:beam_size]\n",
        "      \n",
        "      if top_beam[0][0][0][-1].item() == eos_token_id:\n",
        "        break\n",
        "      # print(top_beam)\n",
        "\n",
        "\n",
        "    summary = tokenizer.decode(top_beam[0][0][0], skip_special_tokens=False)\n",
        "    return summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H36U-rVaPYBQ",
      "metadata": {
        "id": "H36U-rVaPYBQ"
      },
      "source": [
        "- Explain your implementation.\n",
        "- What are the role of hyperparameters of this strategy? and what happen if you change the values?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72276cb9",
      "metadata": {
        "id": "72276cb9"
      },
      "source": [
        "### Beam Search with N-gram Blocking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DHjxN4nIguEV",
      "metadata": {
        "id": "DHjxN4nIguEV"
      },
      "outputs": [],
      "source": [
        "def beam_search_ngram_block(input_ids, beam_size=4, max_length=128, no_repeat_ngram_size=3):\n",
        "    \"\"\"\n",
        "    Perform beam search decoding with n-gram repetition blocking.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tokenized input tensor of shape [1, seq_len].\n",
        "        beam_size (int): Number of beams to explore.\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "        no_repeat_ngram_size (int): Size of n-gram to prevent from repeating.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded summary text.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Beam Search with n-gram blocking\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hBw-4rWiPZTm",
      "metadata": {
        "id": "hBw-4rWiPZTm"
      },
      "source": [
        "- Explain your implementation.\n",
        "- What are the role of hyperparameters of this strategy? and what happen if you change the values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4cecd13",
      "metadata": {
        "id": "e4cecd13"
      },
      "outputs": [],
      "source": [
        "# Example usage with the above decoding functions\n",
        "input_text = dataset[0]['article']\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=1024).input_ids\n",
        "\n",
        "print(\"Greedy Search:\")\n",
        "print(greedy_decode(input_ids))\n",
        "\n",
        "print(\"\\nBeam Search + N-gram Blocking:\")\n",
        "print(beam_search_ngram_block(input_ids))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25fe1341",
      "metadata": {
        "id": "25fe1341"
      },
      "source": [
        "## ROUGE Metric Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac4f0a8",
      "metadata": {
        "id": "5ac4f0a8"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "def compute_rouge_n(reference: str, generated: str, n: int = 1) -> dict:\n",
        "    \"\"\"\n",
        "    Compute ROUGE-N score between reference and generated text.\n",
        "\n",
        "    Args:\n",
        "        reference (str): The reference summary.\n",
        "        generated (str): The generated summary.\n",
        "        n (int): The n-gram size (e.g., 1 for ROUGE-1).\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with 'precision', 'recall', and 'f1' scores.\n",
        "    \"\"\"\n",
        "    # HINT: Implement ROUGE-N calculation using n-gram overlap\n",
        "    pass\n",
        "\n",
        "  def compute_rouge_l(reference: str, generated: str) -> dict:\n",
        "    \"\"\"\n",
        "    Compute ROUGE-L (Longest Common Subsequence) score.\n",
        "\n",
        "    Args:\n",
        "        reference (str): The ground truth summary.\n",
        "        generated (str): The generated summary by the model.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with precision, recall, and f1 scores.\n",
        "    \"\"\"\n",
        "    # HINT: Implement Longest Common Subsequence algorithm\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WRupJH9OPdqJ",
      "metadata": {
        "id": "WRupJH9OPdqJ"
      },
      "source": [
        "- Explanation on your implementation\n",
        "- Discuss how to improve these metrics to perform a better evaluation?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "105bd846",
      "metadata": {
        "id": "105bd846"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f2f8f57",
      "metadata": {
        "id": "6f2f8f57"
      },
      "outputs": [],
      "source": [
        "def generate_summary_custom(article_text, strategy=\"greedy\"):\n",
        "    input_ids = tokenizer(article_text, return_tensors=\"pt\", truncation=True, max_length=1024).input_ids\n",
        "    if strategy == \"greedy\":\n",
        "        return greedy_decode(input_ids)\n",
        "    elif strategy == \"top_k\":\n",
        "        return top_k_decode(input_ids)\n",
        "    elif strategy == \"top_p\":\n",
        "        return top_p_decode(input_ids)\n",
        "    elif strategy == \"beam\":\n",
        "        return beam_search_decode(input_ids)\n",
        "    elif strategy == \"beam_block\":\n",
        "        return beam_search_ngram_block(input_ids)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown decoding strategy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbbb0f60",
      "metadata": {
        "id": "fbbb0f60"
      },
      "outputs": [],
      "source": [
        "# Compare summaries across all decoding strategies for one article\n",
        "sample_article = dataset[0][\"article\"]\n",
        "for strategy in [\"greedy\", \"top_k\", \"top_p\", \"beam\", \"beam_block\"]:\n",
        "    print(f\"\\n[{strategy.upper()}]\")\n",
        "    print(generate_summary_custom(sample_article, strategy=strategy))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RK_9phseyjdx",
      "metadata": {
        "id": "RK_9phseyjdx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Decoding strategies\n",
        "strategies = [\"greedy\", \"top_k\", \"top_p\", \"beam\", \"beam_block\"]\n",
        "\n",
        "# Save evalution results\n",
        "results = {s: [] for s in strategies}\n",
        "\n",
        "# Evaluation loop\n",
        "for i, sample in enumerate(dataset):\n",
        "    article = sample[\"article\"]\n",
        "    reference = sample[\"highlights\"]\n",
        "\n",
        "    input_ids = tokenizer(article, return_tensors=\"pt\", truncation=True, max_length=1024).input_ids\n",
        "\n",
        "    for strategy in strategies:\n",
        "        try:\n",
        "            generated = generate_summary_custom(article, strategy=strategy)\n",
        "\n",
        "            rouge1 = compute_rouge_n(reference, generated, n=1)[\"f1\"]\n",
        "            rouge2 = compute_rouge_n(reference, generated, n=2)[\"f1\"]\n",
        "            rougel = compute_rouge_l(reference, generated)[\"f1\"]\n",
        "\n",
        "            results[strategy].append({\n",
        "                \"rouge1\": rouge1,\n",
        "                \"rouge2\": rouge2,\n",
        "                \"rougeL\": rougel\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"[{strategy}] Error on sample {i}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ox7I3VxhzsbO",
      "metadata": {
        "id": "Ox7I3VxhzsbO"
      },
      "outputs": [],
      "source": [
        "summary = {}\n",
        "for strategy in strategies:\n",
        "    if results[strategy]:\n",
        "        rouge1s = [x[\"rouge1\"] for x in results[strategy]]\n",
        "        rouge2s = [x[\"rouge2\"] for x in results[strategy]]\n",
        "        rougels = [x[\"rougeL\"] for x in results[strategy]]\n",
        "\n",
        "        summary[strategy] = {\n",
        "            \"ROUGE-1\": np.mean(rouge1s),\n",
        "            \"ROUGE-2\": np.mean(rouge2s),\n",
        "            \"ROUGE-L\": np.mean(rougels)\n",
        "        }\n",
        "\n",
        "df = pd.DataFrame(summary).T.sort_values(\"ROUGE-L\", ascending=False)\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fc7f614",
      "metadata": {
        "id": "2fc7f614"
      },
      "source": [
        "## Discussion (Total: 15 points)\n",
        "Answer the following questions based on your decoding outputs and analysis. Be clear and support your claims with examples. You may provide your evidence by implementing additional functions. For example, you may draw a plot or show a statistics.\n",
        "\n",
        "1. Compare the different decoding strategies. Present and justify your findings using examples from your generated summaries. (9 points)\n",
        "  - Which strategies produce more diverse outputs?\n",
        "\n",
        "  - Which ones tend to repeat phrases or truncate early?\n",
        "\n",
        "  - Which are more stable across different runs?\n",
        "\n",
        "\n",
        "2. Analyze the impact of decoding parameters. Suggest complementary evaluation methods that might improve reliability. (3 points)\n",
        "  - How does increasing beam size or sampling temperature affect the results?\n",
        "\n",
        "  - Use at least one example to illustrate the effect.\n",
        "\n",
        "3. Discuss the limitations of ROUGE as an evaluation metric. Suggest complementary evaluation methods that might improve reliability. (3 points)\n",
        "  - What aspects of summarization quality does ROUGE fail to capture?\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
