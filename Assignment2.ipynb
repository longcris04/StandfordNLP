{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e53bf5c7",
      "metadata": {
        "id": "e53bf5c7"
      },
      "source": [
        "# Assignment 2: Implementing Decoding Strategies for Summarization (40 points)\n",
        "\n",
        "In this assignment, you will **implement decoding algorithms** used for text summarization using a pretrained Transformer model.\n",
        "\n",
        "---\n",
        "\n",
        "### Your Task\n",
        "1. **Load** the `sshleifer/distilbart-cnn-12-6` summarization model.\n",
        "2. **Implement the following decoding strategies from scratch** (no `.generate()` allowed!). You need to provide your own explanation on your implementation for each function:\n",
        "   - Greedy decoding (3 points)\n",
        "   - Top-k sampling (3 points)\n",
        "   - Top-p (nucleus) sampling (3 points)\n",
        "   - Beam search (3 points)\n",
        "   - Beam search with n-gram blocking (3 points)\n",
        "3. Use your decoder to summarize 200 articles from the CNN/DailyMail dataset.\n",
        "4. Implement the following ROUGE metrics and evaluate your summaries using your own ROUGE metric implementation.\n",
        "  - Implementation\n",
        "    - ROUGE-n (2 points): e.g., ROUGE-1 & ROUGE-2\n",
        "    - ROUGE-L (4 points)\n",
        "  - Explanation on your implementation (2 points)\n",
        "  - Discuss how to improve these metrics to perform a better evaluation? (2 points)\n",
        "5. Discussion (15 points)\n",
        "\n",
        "---\n",
        "\n",
        "**Note:**\n",
        "  - Regarding the decoding strategies, you are expected to work directly with model logits and sampling logic. Do not use `model.generate()` or any pre-built function. **Hint**: use \"outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\"\n",
        "  - For each function, you need to write clear and concise comments about your implementation. This may not be line-by-line, but rather meaningful chunk of codes.\n",
        "  - For each question, justify your answer with explanations.\n",
        "\n",
        "### **We highly recommend using 'cpu' as the default for development, and switching to 'gpu' only for evaluating summarization performance. This is due to the limited GPU availability in the Colab environment.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f3b788c8",
      "metadata": {
        "id": "f3b788c8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f2b0b6c5",
      "metadata": {
        "id": "f2b0b6c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "073cb56f-6596-472b-e5b1-9a798d0eb271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import json\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "# load model to gpu for faster inference\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71208984",
      "metadata": {
        "id": "71208984"
      },
      "source": [
        "In this assignment, we use the CNN/DailyMail summarization dataset, a widely-used benchmark for training and evaluating text summarization models.\n",
        "\n",
        "Each data sample consists of:\n",
        "\n",
        "- article: A news story, usually between 300 and 800 words.\n",
        "\n",
        "- highlights: A bullet-style abstractive summary of the article, written by human editors.\n",
        "\n",
        "This dataset is ideal for testing decoding strategies because:\n",
        "\n",
        "- The summaries are relatively short and factual\n",
        "\n",
        "- The dataset is large enough to support diverse decoding behavior\n",
        "\n",
        "- It has been used in many summarization papers, making ROUGE scores easy to compare\n",
        "\n",
        "In this assignment, we will use a small subset of 200 samples from the validation set for quick experimentation and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a1d2d74e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "a1d2d74e",
        "outputId": "b4134236-8eaa-4041-be0b-e8d2c8f5e220"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'cnn_dm_200.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-93465f2cd54e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load 200-sample dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cnn_dm_200.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cnn_dm_200.json'"
          ]
        }
      ],
      "source": [
        "# Upload cnn_dm_200.json file\n",
        "\n",
        "# Load 200-sample dataset\n",
        "with open(\"cnn_dm_200.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# because the limited computation resource in google collab, I only use first 10 samples from the dataset to run experiment.\n",
        "dataset = dataset[:10]\n",
        "\n",
        "print(f\"Loaded {len(dataset)} samples.\")\n",
        "print(\"\\nExample article:\")\n",
        "print(dataset[0][\"article\"])\n",
        "print(\"\\nReference summary:\")\n",
        "print(dataset[0][\"highlights\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "491ce644",
      "metadata": {
        "id": "491ce644"
      },
      "source": [
        "## Decoding in Language Models\n",
        "Pretrained language models like BART or GPT generate text by predicting the most likely next token one at a time.\n",
        "However, simply choosing the most probable token at each step often leads to deterministic, repetitive, or generic outputs.\n",
        "\n",
        "To overcome this, several decoding strategies have been proposed to balance:\n",
        "\n",
        "- fluency vs. diversity\n",
        "\n",
        "- coherence vs. novelty\n",
        "\n",
        "In this assignment, you will implement five widely-used decoding strategies from scratch and compare their effects."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75e0c151",
      "metadata": {
        "id": "75e0c151"
      },
      "source": [
        "### Greedy Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "42d72411",
      "metadata": {
        "id": "42d72411"
      },
      "outputs": [],
      "source": [
        "def greedy_decode(input_ids, max_length=30):\n",
        "    \"\"\"\n",
        "    Perform greedy decoding from the model using logits.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tokenized input tensor of shape [1, seq_len].\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded summary text.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Greedy Decoding from scratch\n",
        "    input_ids = input_ids.to(device) # load model to device\n",
        "    decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]]).to(device) # take the start token for decoding\n",
        "    eos_token_id = model.config.eos_token_id # take the EOS token id from the vocab\n",
        "\n",
        "    for _ in range(max_length):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
        "        proba_output = torch.softmax(output.logits[:,-1,:],axis=-1) # take the last token logits and apply softmax to get the probability of each token\n",
        "        id = torch.tensor([[torch.argmax(proba_output, axis=-1)]]).to(device) # greedy approach to take the token_id with largest probability\n",
        "        decoder_input_ids = torch.cat([decoder_input_ids,id],axis=-1) # append the decoded token to the generated token sequence\n",
        "\n",
        "        if id.item() == eos_token_id: # If the model generate an EOS token, the decoding process should be stop\n",
        "            break\n",
        "\n",
        "    summary = tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True) # convert the generated token sequence to string\n",
        "\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AW2YSwP3PGcn",
      "metadata": {
        "id": "AW2YSwP3PGcn"
      },
      "source": [
        "Explain your implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed0b256c",
      "metadata": {
        "id": "ed0b256c"
      },
      "source": [
        "My implementation performs greedy decoding for a sequence-to-sequence model by iteratively generating tokens based on the highest probability prediction at each step. Firstly, it inputs a decoder start token id to start auto-regressive process. The model takes in decoder start token id and returns logits for next token. The softmax function is applied to convert logits to probabilites, and the token id, which is the index in the tensor, is chosen with respect to the highest probability. The chosen token id is then concatenated with the sequence of previously generated tokens, to feed back into the model for the next iteration. The process stop until model outputs the end of sentence token.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dba10a34",
      "metadata": {
        "id": "dba10a34"
      },
      "source": [
        "### Top-k Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "22a1957b",
      "metadata": {
        "id": "22a1957b"
      },
      "outputs": [],
      "source": [
        "def top_k_decode(input_ids, k=5, max_length=30, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Perform Top-k sampling decoding from the model.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tokenized input tensor of shape [1, seq_len].\n",
        "        k (int): Number of top tokens to sample from.\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "        temperature (float): Softmax temperature for sampling.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded summary text.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Top-k Sampling Decoding\n",
        "    input_ids = input_ids.to(device)\n",
        "    decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]]).to(device) # take the start token for decoding\n",
        "    eos_token_id = model.config.eos_token_id # take the EOS token id from the vocab\n",
        "\n",
        "    for _ in range(max_length):\n",
        "      with torch.no_grad():\n",
        "        output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
        "      output_proba = torch.softmax(output.logits[:,-1,:]/temperature,axis=-1) # convert logits to probability, devide by the temperature to control the randomness and shape of probability distribution\n",
        "      values, indices = torch.topk(output_proba, k=k, axis=-1) # take top k tokens with largest probability\n",
        "      values = values/ torch.sum(values,axis=-1,keepdim=True).item() # redistribute probability of selected tokens, made them summing up to 1\n",
        "      local_index = torch.multinomial(values[0],num_samples=1) # sampling from redistributed probability distribution\n",
        "      id = indices[:,local_index] # take the token id of the sampled token\n",
        "      decoder_input_ids = torch.cat([decoder_input_ids,id],axis=-1) # add the sampled token to the generated token sequence\n",
        "\n",
        "      if id.item() == eos_token_id: # If the model generate an EOS token, the decoding process should be stop\n",
        "        break\n",
        "\n",
        "    summary = tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True) # convert the generated token sequence to string\n",
        "    return summary\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oyFRQsTiPWHn",
      "metadata": {
        "id": "oyFRQsTiPWHn"
      },
      "source": [
        "- Explain your implementation.\n",
        "- What are the role of hyperparameters of this strategy? and what happen if you change the values?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aa8530e",
      "metadata": {
        "id": "0aa8530e"
      },
      "source": [
        "- My implementation of top-k sampling involves selecting the top k most probable tokens from the model's output logits at each decoding step. The logits are first converted to probabilities using the softmax function. Then, use torch.topk function to select k indices with largest probabilities. Then, the probabilities of k selected tokens must be redistributed to have sum of 1. After that, I sample a token using torch.multinomial function to sample elements from a given probability distribution. This process is repeated until an end-of-sequence token is generated or a maximum length is reached.\n",
        "- The hyperparameter k determines the number of top tokens to consider for sampling. A smaller k leads to more deterministic outputs, as fewer options are available, while a larger k increases diversity but may introduce noise. If k is set too high, the model may generate less coherent or relevant text, as it has too many options to choose from. Conversely, if k is too low, the generated text may become repetitive or lack variety. If k is set to 1, top k sampling becomes equivalent to greedy decoding, as only the most probable token is selected at each step.\n",
        "- The hyperpatameter temperature is used to control the randomness of the sampling process. A lower temperature results in more conservative and deterministic outputs, while a higher temperature increases diversity but may lead to less coherent text. If the temperature is set to 1, it means no scaling is applied to the logits, and the sampling process behaves as if it were using the original probabilities. If we increase the temperature, the probabilities distribution becomes more uniform. In contrast, we reduce the temperature, the distribution becomes sharper."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b06aad60",
      "metadata": {
        "id": "b06aad60"
      },
      "source": [
        "### Top-p Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "fe9c4772",
      "metadata": {
        "id": "fe9c4772"
      },
      "outputs": [],
      "source": [
        "def top_p_decode(input_ids, p=0.9, max_length=30, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Perform Top-p (nucleus) sampling decoding from the model.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tokenized input tensor of shape [1, seq_len].\n",
        "        p (float): Cumulative probability threshold for sampling.\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "        temperature (float): Softmax temperature for sampling.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded summary text.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Top-p Sampling Decoding\n",
        "\n",
        "    input_ids = input_ids.to(device)\n",
        "    decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]]).to(device) # take the start token for decoding\n",
        "    eos_token_id = model.config.eos_token_id # take the EOS token id from the vocab\n",
        "\n",
        "    for _ in range(max_length):\n",
        "      with torch.no_grad():\n",
        "        output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
        "      output_proba = torch.softmax(output.logits[:,-1,:]/temperature,axis=-1)[0] # convert logits to probability, devide by the temperature to control the randomness and shape of probability distribution\n",
        "      sorted_probs, sorted_indices = torch.sort(output_proba, descending=True) # sort the probabilites and their corresponding token ids\n",
        "      cumulative_probs = torch.cumsum(sorted_probs, dim=0) # calculate the cumulative probability\n",
        "      nucleus_mask = cumulative_probs <= p # create a mask for the tokens that are below the threshold\n",
        "      nucleus_mask[max((cumulative_probs > p).nonzero(as_tuple=True)[0][0], 0)] = True # Ensure we include at least one token after passing threshold\n",
        "      nucleus_probs = sorted_probs[nucleus_mask] # filter the probabilities based on the mask\n",
        "      nucleus_indices = sorted_indices[nucleus_mask] # filter the token ids based on the mask\n",
        "      nucleus_probs = nucleus_probs/torch.sum(nucleus_probs) # redistribute the probabilities of the selected tokens\n",
        "      local_index = torch.multinomial(nucleus_probs,num_samples=1) # sampling from redistributed probability distribution\n",
        "      id = torch.tensor([[nucleus_indices[local_index]]]).to(device)\n",
        "      decoder_input_ids = torch.cat([decoder_input_ids,id],axis=-1)\n",
        "\n",
        "      if id.item() == eos_token_id: # If the model generate an EOS token, the decoding process should be stop\n",
        "        break\n",
        "\n",
        "\n",
        "    summary = tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ynhylQrWPXJi",
      "metadata": {
        "id": "ynhylQrWPXJi"
      },
      "source": [
        "- Explain your implementation.\n",
        "- What are the role of hyperparameters of this strategy? and what happen if you change the values?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "081cae83",
      "metadata": {
        "id": "081cae83"
      },
      "source": [
        "- My implementation of top-p (percentile) decoding is different from top-k sampling in that: It selects the smallest set of tokens whose cumulative probability exceeds a threshold p. The logits are first devided by temperature to control the shape of distribution and converted to probabilities using the softmax function. Then, I sort the probabilities in descending order and compute the cumulative sum. I select the smallest set of tokens such that their cumulative probability is greater than or equal to p, implemented by using masks for filtering in tensor. After that, the probabilities of selected candidates must be redistributed to sum up 1. Afterwards, I sample a token from this set and the remaining process is similar to the top-k sampling method. This process is repeated until an end-of-sequence token is generated or a maximum length is reached.      \n",
        "- The hyperparameter p determines the threshold for cumulative probability. A smaller p leads to more deterministic outputs, as fewer options are available, while a larger p increases diversity but may introduce noise. If p is set too high, the model may generate less coherent or relevant text, as it has too many options to choose from. Conversely, if p is too low, the generated text may become repetitive or lack variety.\n",
        "- The hyperpatameter temperature is used to control the randomness of the sampling process. A lower temperature results in more conservative and deterministic outputs, while a higher temperature increases diversity but may lead to less coherent text. If the temperature is set to 1, it means no scaling is applied to the logits, and the sampling process behaves as if it were using the original probabilities. If we increase the temperature, the probabilities distribution becomes more uniform. In contrast, we reduce the temperature, the distribution becomes sharper.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b274b9f",
      "metadata": {
        "id": "2b274b9f"
      },
      "source": [
        "### Beam Search Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f9fa0a79",
      "metadata": {
        "id": "f9fa0a79"
      },
      "outputs": [],
      "source": [
        "def beam_search_decode(input_ids, beam_size=2, max_length=30):\n",
        "    \"\"\"\n",
        "    Perform beam search decoding from the model.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tokenized input tensor of shape [1, seq_len].\n",
        "        beam_size (int): Number of beams to explore.\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded summary text.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Beam Search Decoding\n",
        "    input_ids = input_ids.to(device)\n",
        "    decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]]).to(device) # take the start token for decoding\n",
        "    eos_token_id = model.config.eos_token_id # take the EOS token id from the vocab\n",
        "    vocab_size = model.config.vocab_size # take the vocab size from the model config\n",
        "\n",
        "    top_beam = [] # to store top k beams with highest probability at each step\n",
        "    with torch.no_grad():\n",
        "      output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
        "\n",
        "    output_proba = torch.softmax(output.logits[:,-1,:],axis=-1)\n",
        "    probas, indices = torch.topk(output_proba,k=beam_size,axis=-1) # select top k tokens with largest probability, called beams in the first step\n",
        "    for index,proba in zip(indices[0],probas[0]):\n",
        "      top_beam.append((torch.tensor([[decoder_input_ids,index]]),proba.item())) # append top k beams to the list to start the beam search process\n",
        "\n",
        "    for _ in range(max_length):\n",
        "      beam_list = [] # beam list to store all the beams, total number of beams in comparison will be beam_size * vocab_size\n",
        "      for beam in top_beam:\n",
        "        with torch.no_grad():\n",
        "          output = model(input_ids=input_ids, decoder_input_ids=beam[0].to(device)).logits[:,-1,:]\n",
        "        output_proba = beam[1] * torch.softmax(output,axis=-1) # calculate the cumulative probability by multiplying the probability of the current beam with the conditional probability of the next token\n",
        "        for idx,local_proba in enumerate(list(output_proba[0])):\n",
        "          beam_list.append((torch.cat([beam[0],torch.tensor([[idx]])],axis=-1), local_proba.item())) # append each candidate to the beam list, the candidate is the current beam with the next token id appended to it\n",
        "\n",
        "      sorted_beam_list = sorted(beam_list, key=lambda item: item[1], reverse=True) # sort the beam list based on the cumulated probability for later selection\n",
        "      top_beam = sorted_beam_list[:beam_size] # select top k beams with largest probability\n",
        "\n",
        "      if top_beam[0][0][0][-1].item() == eos_token_id:\n",
        "        break\n",
        "\n",
        "    summary = tokenizer.decode(top_beam[0][0][0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H36U-rVaPYBQ",
      "metadata": {
        "id": "H36U-rVaPYBQ"
      },
      "source": [
        "- Explain your implementation.\n",
        "- What are the role of hyperparameters of this strategy? and what happen if you change the values?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdc8d94b",
      "metadata": {
        "id": "bdc8d94b"
      },
      "source": [
        "- My implementation of beam search decoding involves maintaining a fixed number of candidate sequences (beams) at each decoding step. The process starts with the decoder start token id and iteratively expands each beam by generating the next token probabilities. For each beam, I compute the probabilities of all possible next tokens and select the top k beams based on their cumulative probabilities. This is done using append all possible beams to the list and sorted based on their probabilities. The process continues until an end-of-sequence token is generated and is selected as the highest probable beam or a maximum length is reached.\n",
        "- The hyperparameter k (beam width) determines the number of candidate sequences to maintain at each step. A larger k increases the search space and may result in more diverse output as it can explore beam with higher probability. This may leads to more coherent outputs, but also increases computational cost. A smaller k reduces the search space and may lead to less diverse outputs. If k is set to 1, beam search becomes equivalent to greedy decoding, as only the most probable sequence is selected at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72276cb9",
      "metadata": {
        "id": "72276cb9"
      },
      "source": [
        "### Beam Search with N-gram Blocking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "cacdfda9",
      "metadata": {
        "id": "cacdfda9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def isBlocked(token_seq: list(), no_repeat_ngram_size: int, next_token: int):\n",
        "  '''\n",
        "  This function checks if the next token is blocked by the n-gram repetition blocking rule.\n",
        "  It iteratively check if the next token is in the last n-gram size of the token sequence.\n",
        "  '''\n",
        "  if len(token_seq) < no_repeat_ngram_size:\n",
        "    return False\n",
        "  candidate_seq = token_seq[-(no_repeat_ngram_size-1):] + [next_token]\n",
        "\n",
        "  for i in range(len(token_seq)-(no_repeat_ngram_size-1)):\n",
        "    ref_seq = token_seq[i: i+ no_repeat_ngram_size]\n",
        "    if ref_seq == candidate_seq:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def beam_search_ngram_block(input_ids, beam_size=2, max_length=30, no_repeat_ngram_size=3):\n",
        "    \"\"\"\n",
        "    Perform beam search decoding with n-gram repetition blocking.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tokenized input tensor of shape [1, seq_len].\n",
        "        beam_size (int): Number of beams to explore.\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "        no_repeat_ngram_size (int): Size of n-gram to prevent from repeating.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded summary text.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Beam Search with n-gram blocking\n",
        "\n",
        "\n",
        "    input_ids = input_ids.to(device)\n",
        "    decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]]).to(device)\n",
        "    eos_token_id = model.config.eos_token_id # take the EOS token id from the vocab\n",
        "    vocab_size = model.config.vocab_size # take the vocab size from the model config\n",
        "\n",
        "    top_beam = []\n",
        "    with torch.no_grad():\n",
        "      output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids).logits[:,-1,:]\n",
        "    output_proba = torch.softmax(output,axis=-1)\n",
        "    probas, indices = torch.topk(output_proba,k=beam_size,axis=-1)\n",
        "    for index,proba in zip(indices[0],probas[0]):\n",
        "      top_beam.append((torch.tensor([[decoder_input_ids,index]]),proba.item()))\n",
        "\n",
        "    for _ in range(max_length):\n",
        "      beam_list = []\n",
        "      for beam in top_beam:\n",
        "        with torch.no_grad():\n",
        "          output = model(input_ids=input_ids, decoder_input_ids=beam[0].to(device))\n",
        "        output = output.logits[:,-1,:]\n",
        "        output_proba = beam[1] * torch.softmax(output,axis=-1)\n",
        "        for idx,local_proba in enumerate(list(output_proba[0])):\n",
        "          if not isBlocked(token_seq=beam[0][0].tolist(),no_repeat_ngram_size=no_repeat_ngram_size,next_token=idx): # check if the next token is blocked by the n-gram repetition blocking rule before being added to the beam list\n",
        "            beam_list.append((torch.cat([beam[0],torch.tensor([[idx]])],axis=-1), local_proba.item()))\n",
        "      sorted_beam_list = sorted(beam_list, key=lambda item: item[1], reverse=True)\n",
        "      top_beam = sorted_beam_list[:beam_size]\n",
        "\n",
        "      if top_beam[0][0][0][-1].item() == eos_token_id:\n",
        "        break\n",
        "\n",
        "    summary = tokenizer.decode(top_beam[0][0][0], skip_special_tokens=True)\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hBw-4rWiPZTm",
      "metadata": {
        "id": "hBw-4rWiPZTm"
      },
      "source": [
        "- Explain your implementation.\n",
        "- What are the role of hyperparameters of this strategy? and what happen if you change the values?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88fcc4a",
      "metadata": {
        "id": "b88fcc4a"
      },
      "source": [
        "- My implementation for beam search with n-gram blocking is almost similar to the implementation of beam search decoding, except the checking of valid beams. before the candidate beam is added to the list, it should be checked if it creates n-grams that are already present in the previous beams. If it does, it is discarded.\n",
        "- The hyperparameter n (no_repeat_ngram_size) determines the size of the n-grams to block. A larger n leads to less aggressive blocking as only large n-grams are blocked, allowing short repetition. A smaller n avoids more repetition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e4cecd13",
      "metadata": {
        "id": "e4cecd13",
        "outputId": "f547a954-7e60-4292-c6d3-291445eefedc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy Search:\n",
            " The Palestinian Authority becomes the 123rd member of the International Criminal Court . The move gives the court jurisdiction over alleged crimes in Palestinian territories . Israel and the\n",
            "\n",
            "Top k decoding:\n",
            " The Palestinian Authority is officially the 123rd member of the Criminal Court . It has signed the Rome Statute and accepted its jurisdiction in the Palestinian territory .\n",
            "\n",
            "Top p decoding:\n",
            " ICC has jurisdiction over alleged crimes in Palestinian territories . The International Criminal Court was set up in 2002 .\n",
            "\n",
            "Beam search decoding:\n",
            " The Palestinian Authority becomes the 123rd member of the International Criminal Court . The move gives the court jurisdiction over alleged crimes in Palestinian territories . Israel and the United\n",
            "\n",
            "Beam Search + N-gram Blocking:\n",
            " The Palestinian Authority becomes the 123rd member of the International Criminal Court . The move gives the court jurisdiction over alleged crimes in Palestinian territories . Israel and the United\n",
            "Execution time (in seconds): 288.24523997306824\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# Example usage with the above decoding functions\n",
        "input_text = dataset[0]['article']\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=1024).input_ids\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"Greedy Search:\")\n",
        "print(greedy_decode(input_ids))\n",
        "\n",
        "print(\"\\nTop k decoding:\")\n",
        "print(top_k_decode(input_ids))\n",
        "\n",
        "print(\"\\nTop p decoding:\")\n",
        "print(top_p_decode(input_ids))\n",
        "\n",
        "print(\"\\nBeam search decoding:\")\n",
        "print(beam_search_decode(input_ids))\n",
        "\n",
        "print(\"\\nBeam Search + N-gram Blocking:\")\n",
        "print(beam_search_ngram_block(input_ids))\n",
        "\n",
        "end_time = time.time()\n",
        "excecution_time = end_time - start_time\n",
        "print(f\"Execution time (in seconds): {excecution_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25fe1341",
      "metadata": {
        "id": "25fe1341"
      },
      "source": [
        "## ROUGE Metric Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4bacaa3b",
      "metadata": {
        "id": "4bacaa3b"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "def compute_rouge_n(reference: str, generated: str, n: int = 1) -> dict:\n",
        "    \"\"\"\n",
        "    Compute ROUGE-N score between reference and generated text.\n",
        "\n",
        "    Args:\n",
        "        reference (str): The reference summary.\n",
        "        generated (str): The generated summary.\n",
        "        n (int): The n-gram size (e.g., 1 for ROUGE-1).\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with 'precision', 'recall', and 'f1' scores.\n",
        "    \"\"\"\n",
        "    ref_tokens = tokenize(reference)\n",
        "    gen_tokens = tokenize(generated)\n",
        "\n",
        "    ref_ngrams = Counter([tuple(ref_tokens[i:i+n]) for i in range(len(ref_tokens)-n+1)])\n",
        "    gen_ngrams = Counter([tuple(gen_tokens[i:i+n]) for i in range(len(gen_tokens)-n+1)])\n",
        "\n",
        "\n",
        "    overlap = sum((ref_ngrams & gen_ngrams).values())\n",
        "\n",
        "    precision = overlap / sum(gen_ngrams.values()) if gen_ngrams else 0.0\n",
        "    recall = overlap / sum(ref_ngrams.values()) if ref_ngrams else 0.0\n",
        "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    return {'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "def lcs(X, Y):\n",
        "    \"\"\"\n",
        "    Helper function to compute the length of Longest Common Subsequence (LCS)\n",
        "    \"\"\"\n",
        "    m, n = len(X), len(Y)\n",
        "    dp = [[0] * (n+1) for _ in range(m+1)]\n",
        "\n",
        "    for i in range(m):\n",
        "        for j in range(n):\n",
        "            if X[i] == Y[j]:\n",
        "                dp[i+1][j+1] = dp[i][j] + 1\n",
        "            else:\n",
        "                dp[i+1][j+1] = max(dp[i][j+1], dp[i+1][j])\n",
        "    return dp[m][n]\n",
        "\n",
        "def compute_rouge_l(reference: str, generated: str) -> dict:\n",
        "    \"\"\"\n",
        "    Compute ROUGE-L (Longest Common Subsequence) score.\n",
        "\n",
        "    Args:\n",
        "        reference (str): The ground truth summary.\n",
        "        generated (str): The generated summary by the model.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with precision, recall, and f1 scores.\n",
        "    \"\"\"\n",
        "    ref_tokens = tokenize(reference)\n",
        "    gen_tokens = tokenize(generated)\n",
        "\n",
        "    lcs_len = lcs(ref_tokens, gen_tokens)\n",
        "\n",
        "    precision = lcs_len / len(gen_tokens) if gen_tokens else 0.0\n",
        "    recall = lcs_len / len(ref_tokens) if ref_tokens else 0.0\n",
        "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    return {'precision': precision, 'recall': recall, 'f1': f1}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WRupJH9OPdqJ",
      "metadata": {
        "id": "WRupJH9OPdqJ"
      },
      "source": [
        "- Explanation on your implementation\n",
        "- Discuss how to improve these metrics to perform a better evaluation?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "105bd846",
      "metadata": {
        "id": "105bd846"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6f2f8f57",
      "metadata": {
        "id": "6f2f8f57"
      },
      "outputs": [],
      "source": [
        "def generate_summary_custom(article_text, strategy=\"greedy\"):\n",
        "    input_ids = tokenizer(article_text, return_tensors=\"pt\", truncation=True, max_length=1024).input_ids\n",
        "    if strategy == \"greedy\":\n",
        "        return greedy_decode(input_ids)\n",
        "    elif strategy == \"top_k\":\n",
        "        return top_k_decode(input_ids)\n",
        "    elif strategy == \"top_p\":\n",
        "        return top_p_decode(input_ids)\n",
        "    elif strategy == \"beam\":\n",
        "        return beam_search_decode(input_ids)\n",
        "    elif strategy == \"beam_block\":\n",
        "        return beam_search_ngram_block(input_ids)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown decoding strategy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "fbbb0f60",
      "metadata": {
        "id": "fbbb0f60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "271a1f7f-ceee-4aa4-b4cd-390d7bf27553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[GREEDY]\n",
            " The Palestinian Authority becomes the 123rd member of the International Criminal Court . The move gives the court jurisdiction over alleged crimes in Palestinian territories . Israel and the\n",
            "\n",
            "[TOP_K]\n",
            " The Palestinian Authority formally joins the ICC in a ceremony in the Hague . It gives the court jurisdiction over alleged crimes in Palestinian territories . Palestinians signed the Rome\n",
            "\n",
            "[TOP_P]\n",
            " Palestinian Authority formally becomes 123rd member of the International Criminal Court . The move means the court can investigate alleged crimes committed by Palestinians in Palestinian territories . Israel\n",
            "\n",
            "[BEAM]\n",
            " The Palestinian Authority becomes the 123rd member of the International Criminal Court . The move gives the court jurisdiction over alleged crimes in Palestinian territories . Israel and the United\n",
            "\n",
            "[BEAM_BLOCK]\n",
            " The Palestinian Authority becomes the 123rd member of the International Criminal Court . The move gives the court jurisdiction over alleged crimes in Palestinian territories . Israel and the United\n"
          ]
        }
      ],
      "source": [
        "# Compare summaries across all decoding strategies for one article\n",
        "sample_article = dataset[0][\"article\"]\n",
        "for strategy in [\"greedy\", \"top_k\", \"top_p\", \"beam\", \"beam_block\"]:\n",
        "    print(f\"\\n[{strategy.upper()}]\")\n",
        "    print(generate_summary_custom(sample_article, strategy=strategy))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "RK_9phseyjdx",
      "metadata": {
        "id": "RK_9phseyjdx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Decoding strategies\n",
        "strategies = [\"greedy\", \"top_k\", \"top_p\", \"beam\", \"beam_block\"]\n",
        "\n",
        "# Save evalution results\n",
        "results = {s: [] for s in strategies}\n",
        "\n",
        "# Evaluation loop\n",
        "for i, sample in enumerate(dataset):\n",
        "    article = sample[\"article\"]\n",
        "    reference = sample[\"highlights\"]\n",
        "\n",
        "    input_ids = tokenizer(article, return_tensors=\"pt\", truncation=True, max_length=1024).input_ids\n",
        "\n",
        "    for strategy in strategies:\n",
        "        try:\n",
        "            generated = generate_summary_custom(article, strategy=strategy)\n",
        "\n",
        "            rouge1 = compute_rouge_n(reference, generated, n=1)[\"f1\"]\n",
        "            rouge2 = compute_rouge_n(reference, generated, n=2)[\"f1\"]\n",
        "            rougel = compute_rouge_l(reference, generated)[\"f1\"]\n",
        "\n",
        "            results[strategy].append({\n",
        "                \"rouge1\": rouge1,\n",
        "                \"rouge2\": rouge2,\n",
        "                \"rougeL\": rougel\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"[{strategy}] Error on sample {i}: {e}\")\n",
        "\n",
        "# code to save results\n",
        "with open(\"evaluation_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ox7I3VxhzsbO",
      "metadata": {
        "id": "Ox7I3VxhzsbO"
      },
      "outputs": [],
      "source": [
        "summary = {}\n",
        "for strategy in strategies:\n",
        "    if results[strategy]:\n",
        "        rouge1s = [x[\"rouge1\"] for x in results[strategy]]\n",
        "        rouge2s = [x[\"rouge2\"] for x in results[strategy]]\n",
        "        rougels = [x[\"rougeL\"] for x in results[strategy]]\n",
        "\n",
        "        summary[strategy] = {\n",
        "            \"ROUGE-1\": np.mean(rouge1s),\n",
        "            \"ROUGE-2\": np.mean(rouge2s),\n",
        "            \"ROUGE-L\": np.mean(rougels)\n",
        "        }\n",
        "\n",
        "df = pd.DataFrame(summary).T.sort_values(\"ROUGE-L\", ascending=False)\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fc7f614",
      "metadata": {
        "id": "2fc7f614"
      },
      "source": [
        "## Discussion (Total: 15 points)\n",
        "Answer the following questions based on your decoding outputs and analysis. Be clear and support your claims with examples. You may provide your evidence by implementing additional functions. For example, you may draw a plot or show a statistics.\n",
        "\n",
        "1. Compare the different decoding strategies. Present and justify your findings using examples from your generated summaries. (9 points)\n",
        "  - Which strategies produce more diverse outputs?\n",
        "\n",
        "  - Which ones tend to repeat phrases or truncate early?\n",
        "\n",
        "  - Which are more stable across different runs?\n",
        "\n",
        "\n",
        "2. Analyze the impact of decoding parameters. Suggest complementary evaluation methods that might improve reliability. (3 points)\n",
        "  - How does increasing beam size or sampling temperature affect the results?\n",
        "\n",
        "  - Use at least one example to illustrate the effect.\n",
        "\n",
        "3. Discuss the limitations of ROUGE as an evaluation metric. Suggest complementary evaluation methods that might improve reliability. (3 points)\n",
        "  - What aspects of summarization quality does ROUGE fail to capture?\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deepseek_ft",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}